{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 22:08:55.064984: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-06 22:08:59.582776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets\n",
    "\n",
    "loading from `/home/nfs/inf6/data/datasets/kth_actions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define the split for training and eval sets\n",
    "train_persons = {'person11', 'person12', 'person13', 'person14', 'person15', 'person16', 'person17', 'person02', 'person03', 'person05', 'person06', 'person07', 'person08', 'person09', 'person10', 'person18'}\n",
    "val_persons = {'person19', 'person20', 'person21', 'person23', 'person24', 'person25', 'person01', 'person04'}\n",
    "\n",
    "def get_sequences_with_labels(base_dir, persons):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    classes = os.listdir(base_dir)\n",
    "    class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        for person in persons:\n",
    "            person_sequences = glob(os.path.join(base_dir, cls, f'{person}*'))\n",
    "            sequences.extend(person_sequences)\n",
    "            labels.extend([class_to_idx[cls]] * len(person_sequences))\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "base_dir = '/home/nfs/inf6/data/datasets/kth_actions/processed'\n",
    "train_sequences, train_labels = get_sequences_with_labels(base_dir, train_persons)\n",
    "val_sequences, val_labels = get_sequences_with_labels(base_dir, val_persons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_frames(sequence_path):\n",
    "    frame_files = sorted(glob(os.path.join(sequence_path, '*.png')))\n",
    "    frames = [cv2.imread(frame_file) for frame_file in frame_files]\n",
    "    frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in frames]  # Convert to RGB\n",
    "    frames = [frame / 255.0 for frame in frames]  # Normalize\n",
    "    return frames\n",
    "\n",
    "def create_subsequences(frames, subsequence_length=10):\n",
    "    subsequences = []\n",
    "    for i in range(len(frames) - subsequence_length + 1):\n",
    "        subsequences.append(frames[i:i + subsequence_length])\n",
    "    return subsequences\n",
    "\n",
    "sequence_path = train_sequences[0]\n",
    "frames = load_frames(sequence_path)\n",
    "subsequences = create_subsequences(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, transform=None, subsequence_length=13):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.subsequence_length = subsequence_length\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        data = []\n",
    "        for sequence, label in zip(self.sequences, self.labels):\n",
    "            frames = load_frames(sequence)\n",
    "            subsequences = create_subsequences(frames, self.subsequence_length)\n",
    "            for subsequence in subsequences:\n",
    "                data.append((subsequence, label))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subsequence, label = self.data[idx]\n",
    "        if self.transform:\n",
    "            subsequence = [self.transform(frame) for frame in subsequence]\n",
    "        subsequence = torch.stack([frame for frame in subsequence])  # Convert to CxHxW\n",
    "        label = torch.tensor(label)\n",
    "        return subsequence, label\n",
    "\n",
    "# Define transforms if needed\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.Resize(256),\n",
    "        # transforms.CenterCrop(224),\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = VideoDataset(train_sequences, train_labels, transform=data_transforms['train'])\n",
    "val_dataset = VideoDataset(val_sequences, val_labels, transform=data_transforms['val'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = 4, pin_memory=True)\n",
    "test_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = 4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first item in the first batch: torch.Size([8, 13, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "train_dataset[0][0].size()\n",
    "first_batch = next(iter(train_loader))\n",
    "\n",
    "# Extract the first item in the batch\n",
    "first_item = first_batch[0] # first_batch[0] is the data, first_batch[1] is the labels\n",
    "\n",
    "# Print the shape of the first item\n",
    "print(\"Shape of the first item in the first batch:\", first_item.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom LSTM  \n",
    "Using referred source as reference to build the LSTM cell. The reset parameters functions is required to get the correct distribution for the tensors, which prevents vanishing/exploding gradients or unwanted outcomes in the weights and biases.\n",
    "\n",
    "Reference: https://github.com/piEsposito/pytorch-lstm-by-hand/blob/master/LSTM.ipynb - By Piero Esposito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Forget gate weights\n",
    "        self.W_f = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Input gate weights\n",
    "        self.W_i = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Candidate cell state weights\n",
    "        self.W_c = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Output gate weights\n",
    "        self.W_o = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.W_f, a=0.01)\n",
    "        nn.init.kaiming_uniform_(self.W_i, a=0.01)\n",
    "        nn.init.kaiming_uniform_(self.W_c, a=0.01)\n",
    "        nn.init.kaiming_uniform_(self.W_o, a=0.01)\n",
    "        nn.init.constant_(self.b_f, 0)\n",
    "        nn.init.constant_(self.b_i, 0)\n",
    "        nn.init.constant_(self.b_c, 0)\n",
    "        nn.init.constant_(self.b_o, 0)\n",
    "\n",
    "    def forward(self, input, hx):\n",
    "        # if mode == \"zeroes\":\n",
    "        #     hx = (torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device),\n",
    "        #           torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device))\n",
    "        # elif mode == \"random\":\n",
    "        #     hx = (torch.rand(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device),\n",
    "        #           torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device))\n",
    "\n",
    "\n",
    "        h_prev, c_prev = hx\n",
    "        combined = torch.cat((input, h_prev), dim=1)\n",
    "\n",
    "        # Forget gate\n",
    "        f = torch.sigmoid(torch.matmul(combined, self.W_f) + self.b_f)\n",
    "\n",
    "        # Input gate\n",
    "        i = torch.sigmoid(torch.matmul(combined, self.W_i) + self.b_i)\n",
    "\n",
    "        # Candidate cell state\n",
    "        c_hat = torch.tanh(torch.matmul(combined, self.W_c) + self.b_c)\n",
    "\n",
    "        # Updated cell state\n",
    "        c = f * c_prev + i * c_hat\n",
    "\n",
    "        # Output gate\n",
    "        o = torch.sigmoid(torch.matmul(combined, self.W_o) + self.b_o)\n",
    "\n",
    "        # Updated hidden state\n",
    "        h = o * torch.tanh(c)\n",
    "\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Model using LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialClassifierWithCells(nn.Module):\n",
    "    \"\"\" \n",
    "    Sequential classifier for images. Embedded image rows are fed to a RNN\n",
    "    Same as above, but using LSTMCells instead of the LSTM object\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    input_dim: integer\n",
    "        dimensionality of the rows to embed\n",
    "    emb_dim: integer \n",
    "        dimensionality of the vectors fed to the LSTM\n",
    "    hidden_dim: integer\n",
    "        dimensionality of the states in the cell\n",
    "    mode: string\n",
    "        intialization of the states\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers=1, mode=\"zeros\"):\n",
    "        \"\"\" Module initializer \"\"\"\n",
    "        assert mode in [\"zeros\", \"random\"]\n",
    "        super().__init__()\n",
    "        self.hidden_dim =  hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.mode = mode\n",
    "\n",
    "        # for embedding rows into vector representations\n",
    "        self.encoder = nn.Linear(in_features=input_dim, out_features=emb_dim)\n",
    "        \n",
    "        # LSTM model       \n",
    "        lstms = []\n",
    "        for i in range(num_layers):\n",
    "            in_size = emb_dim if i == 0 else hidden_dim\n",
    "            lstms.append( CustomLSTMCell(input_size=in_size, hidden_size=hidden_dim) )\n",
    "        self.lstm = nn.ModuleList(lstms)\n",
    "        \n",
    "        # FC-classifier\n",
    "        self.classifier = nn.Linear(in_features=hidden_dim, out_features=6)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through model \"\"\"\n",
    "        \n",
    "        b_size, seq_size, n_channels, n_rows, n_cols = x.shape\n",
    "        h, c = self.init_state(b_size=b_size, device=x.device) \n",
    "        \n",
    "        #embedding frames\n",
    "        x = x.view(b_size * seq_size, n_channels * n_rows * n_cols)  # Flatten each image\n",
    "        embeddings = self.encoder(x)  # Apply encoder\n",
    "        embeddings = embeddings.view(b_size, seq_size, -1)  # Reshape back to (batch_size, seq_size, emb_dim)\n",
    "        \n",
    "        # iterating over sequence length\n",
    "        lstm_out = []\n",
    "        for i in range(embeddings.shape[1]):\n",
    "            lstm_input = embeddings[:, i, :]\n",
    "            # iterating over LSTM Cells\n",
    "            for j, lstm_cell in enumerate(self.lstm):\n",
    "                h[j], c[j] = lstm_cell(lstm_input, (h[j], c[j]))\n",
    "                lstm_input = h[j]\n",
    "            lstm_out.append(lstm_input)\n",
    "        lstm_out = torch.stack(lstm_out, dim=1)\n",
    "            \n",
    "        # classifying\n",
    "        y = self.classifier(lstm_out[:, -1, :])  # feeding only output at last layer\n",
    "        \n",
    "        return y\n",
    "    \n",
    "        \n",
    "    def init_state(self, b_size, device):\n",
    "        \"\"\" Initializing hidden and cell state \"\"\"\n",
    "        if(self.mode == \"zeros\"):\n",
    "            h = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        elif(self.mode == \"random\"):\n",
    "            h = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Encoder - extracting features from the frames before being fed as a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=3, feature_dim=64):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        \n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=5, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, feature_dim, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # Define pooling layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Applying convolutional layers and pooling\n",
    "        self.model = nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.ReLU(),\n",
    "            self.pool,\n",
    "            self.conv2,\n",
    "            nn.ReLU(),\n",
    "            self.pool,\n",
    "            # self.conv3,\n",
    "            # nn.ReLU(),\n",
    "            # self.pool\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional LSTM cell. Using convolutional operations to replace the units of the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        \n",
    "        # Define convolutional operations for input, forget, output gates, and cell state\n",
    "        self.conv_i = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
    "                                out_channels=hidden_channels,\n",
    "                                kernel_size=kernel_size,\n",
    "                                padding=self.padding)\n",
    "        self.conv_f = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
    "                                out_channels=hidden_channels,\n",
    "                                kernel_size=kernel_size,\n",
    "                                padding=self.padding)\n",
    "        self.conv_c = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
    "                                out_channels=hidden_channels,\n",
    "                                kernel_size=kernel_size,\n",
    "                                padding=self.padding)\n",
    "        self.conv_o = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
    "                                out_channels=hidden_channels,\n",
    "                                kernel_size=kernel_size,\n",
    "                                padding=self.padding)\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        h_prev, c_prev = hidden_state\n",
    "        \n",
    "        combined_input = torch.cat((x, h_prev), dim=1)\n",
    "        \n",
    "        input_gate = torch.sigmoid(self.conv_i(combined_input))\n",
    "        forget_gate = torch.sigmoid(self.conv_f(combined_input))\n",
    "        output_gate = torch.sigmoid(self.conv_o(combined_input))\n",
    "        \n",
    "        cell_state_candidate = torch.tanh(self.conv_c(combined_input))\n",
    "        \n",
    "        cell_state = forget_gate * c_prev + input_gate * cell_state_candidate\n",
    "        hidden_state = output_gate * torch.tanh(cell_state)\n",
    "        \n",
    "        return hidden_state, cell_state\n",
    "    \n",
    "    def init_hidden(self, batch_size, c, h, w):\n",
    "        return (torch.zeros(batch_size, c, h, w, device=self.conv_lstm.conv_i.weight.device),\n",
    "                torch.zeros(batch_size, c, h, w, device=self.conv_lstm.conv_i.weight.device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We set up a module that uses the ConvLSTM cell to keep track of sequential information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentModule(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(RecurrentModule, self).__init__()\n",
    "        self.conv_lstm = ConvLSTMCell(input_channels, hidden_channels, kernel_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        h_t, c_t = self.init_hidden(batch_size, c, h, w)\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.conv_lstm(x[:, t, :, :, :], (h_t, c_t))\n",
    "            outputs.append(h_t)\n",
    "            \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs[:, -1, :, :, :]\n",
    "    \n",
    "    def init_hidden(self, batch_size, c, h, w):\n",
    "        return (torch.zeros(batch_size, c, h, w, device=self.conv_lstm.conv_i.weight.device),\n",
    "                torch.zeros(batch_size, c, h, w, device=self.conv_lstm.conv_i.weight.device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a model that encapsulates (encoder, ConvLSTM, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 6])\n"
     ]
    }
   ],
   "source": [
    "class ActionRecognitionModel(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, feature_dim, output_dim):\n",
    "        super(ActionRecognitionModel, self).__init__()\n",
    "        \n",
    "        self.encoder = ConvEncoder(input_channels=input_channels, feature_dim=feature_dim)\n",
    "        self.conv_lstm = ConvLSTMCell(input_channels=feature_dim, hidden_channels=hidden_channels, kernel_size=kernel_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_channels, output_dim)\n",
    "        )\n",
    "        \n",
    "    def init_hidden(self, batch_size, hidden_channels, spatial_size):\n",
    "        height, width = spatial_size\n",
    "        h = torch.zeros(batch_size, hidden_channels, height, width, device=next(self.parameters()).device)\n",
    "        c = torch.zeros(batch_size, hidden_channels, height, width, device=next(self.parameters()).device)\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        c_in = x.view(batch_size * seq_len, channels, height, width)\n",
    "        \n",
    "        encoded = self.encoder(c_in)\n",
    "        encoded = encoded.view(batch_size, seq_len, encoded.size(1), encoded.size(2), encoded.size(3))\n",
    "        \n",
    "        h, c = self.init_hidden(batch_size, self.conv_lstm.hidden_channels, (encoded.size(3), encoded.size(4)))\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            h, c = self.conv_lstm(encoded[:, t, :, :, :], (h, c))\n",
    "            outputs.append(h)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        outputs = outputs[:, -1, :, :, :]\n",
    "        \n",
    "        output = self.classifier(outputs)\n",
    "        return output\n",
    "\n",
    "model = ActionRecognitionModel(input_channels=3, hidden_channels=128, kernel_size=3, feature_dim=64, output_dim=6)\n",
    "input_tensor = torch.randn(8, 15, 3, 64, 64)  # Batch of 8, sequence length of 15, 3 channels, 64x64 image\n",
    "output = model(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, epoch, device):\n",
    "    \"\"\" Training a model for one epoch \"\"\"\n",
    "    \n",
    "    loss_list = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            # Forward pass to get output/logits\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate Loss: softmax --> cross entropy loss\n",
    "            loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "         \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        scaler.scale(loss).backward()\n",
    "         \n",
    "        # Updating parameters\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "    mean_loss = np.mean(loss_list)\n",
    "    return mean_loss, loss_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, eval_loader, criterion, device):\n",
    "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_list = []\n",
    "    with autocast():\n",
    "        for images, labels in eval_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass only to get logits/output\n",
    "            outputs = model(images)\n",
    "                    \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(loss.item())\n",
    "                \n",
    "            # Get predictions from the maximum value\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += len( torch.where(preds==labels)[0] )\n",
    "            total += len(labels)\n",
    "                 \n",
    "    # Total correct predictions and loss\n",
    "    accuracy = correct / total * 100\n",
    "    loss = np.mean(loss_list)\n",
    "    \n",
    "    return accuracy, loss\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs, tboard=None, start_epoch=0):\n",
    "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    val_loss =  []\n",
    "    loss_iters = []\n",
    "    valid_acc = []\n",
    "    assert tboard is not None, f\"Tensorboard must be provided!\"\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "           \n",
    "        # validation epoch\n",
    "        model.eval()  # important for dropout and batch norms\n",
    "        accuracy, loss = eval_model(\n",
    "                    model=model, eval_loader=valid_loader,\n",
    "                    criterion=criterion, device=device\n",
    "            )\n",
    "        valid_acc.append(accuracy)\n",
    "        val_loss.append(loss)\n",
    "        tboard.add_scalar(f'Accuracy/Valid', accuracy, global_step=epoch+start_epoch)\n",
    "        tboard.add_scalar(f'Loss/Valid', loss, global_step=epoch+start_epoch)\n",
    "        \n",
    "        # training epoch\n",
    "        model.train()  # important for dropout and batch norms\n",
    "        mean_loss, cur_loss_iters = train_epoch(\n",
    "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
    "                criterion=criterion, epoch=epoch, device=device\n",
    "            )\n",
    "        scheduler.step()\n",
    "        train_loss.append(mean_loss)\n",
    "        tboard.add_scalar(f'Loss/Train', mean_loss, global_step=epoch+start_epoch)\n",
    "\n",
    "        loss_iters = loss_iters + cur_loss_iters\n",
    "        \n",
    "        if(epoch % 5 == 0 or epoch==num_epochs-1):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
    "            print(f\"    Valid loss: {round(loss, 5)}\")\n",
    "            print(f\"    Accuracy: {accuracy}%\")\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    print(f\"Training completed\")\n",
    "    return train_loss, val_loss, loss_iters, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, stats, path=\"\", name=\"\"):\n",
    "    \"\"\" Saving model checkpoint \"\"\"\n",
    "    \n",
    "    if(not os.path.exists(path+\"models\")):\n",
    "        os.makedirs(path+\"models\")\n",
    "    savepath = f\"{path}models/{name}_checkpoint_epoch_{epoch}.pth\"\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'stats': stats\n",
    "    }, savepath)\n",
    "    return\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, savepath):\n",
    "    \"\"\" Loading pretrained checkpoint \"\"\"\n",
    "    \n",
    "    checkpoint = torch.load(savepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    stats = checkpoint[\"stats\"]\n",
    "    \n",
    "    return model, optimizer, epoch, stats\n",
    "\n",
    "\n",
    "def smooth(f, K=5):\n",
    "    \"\"\" Smoothing a function using a low-pass filter (mean) of size K \"\"\"\n",
    "    kernel = np.ones(K) / K\n",
    "    f = np.concatenate([f[:int(K//2)], f, f[int(-K//2):]])  # to account for boundaries\n",
    "    smooth_f = np.convolve(f, kernel, mode=\"same\")\n",
    "    smooth_f = smooth_f[K//2: -K//2]  # removing boundary-fixes\n",
    "    return smooth_f\n",
    "\n",
    "def set_random_seed(random_seed=None):\n",
    "    \"\"\"\n",
    "    Using random seed for numpy and torch\n",
    "    \"\"\"\n",
    "    if(random_seed is None):\n",
    "        random_seed = 13\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    return\n",
    "\n",
    "\n",
    "def count_model_params(model):\n",
    "    \"\"\" Counting the number of learnable parameters in a nn.Module \"\"\"\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1189766"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialClassifierWithCells(input_dim=3*32*32, emb_dim=6000, hidden_dim=128, num_layers=2, mode=\"zeros\")\n",
    "count_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialClassifierWithCells(\n",
       "  (encoder): Linear(in_features=3072, out_features=6000, bias=True)\n",
       "  (lstm): ModuleList(\n",
       "    (0-1): 2 x CustomLSTMCell()\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 5 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBOARD_LOGS = os.path.join(os.getcwd(),\"Lab Work\", \"CUDA_Assignment_4\", \"tboard_logs\", \"RNNs\", \"CustomLSTMcells\")\n",
    "if not os.path.exists(TBOARD_LOGS):\n",
    "    os.makedirs(TBOARD_LOGS)\n",
    "\n",
    "shutil.rmtree(TBOARD_LOGS)\n",
    "writer = SummaryWriter(TBOARD_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████▍                                                                                         | 1/5 [09:58<39:54, 598.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    Train loss: 1.45161\n",
      "    Valid loss: 1.81185\n",
      "    Accuracy: 13.313790987797397%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [50:10<00:00, 602.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "    Train loss: 1.23768\n",
      "    Valid loss: 1.3696\n",
      "    Accuracy: 41.16049719362829%\n",
      "\n",
      "\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss, loss_iters, valid_acc = train_model(\n",
    "        model=model, optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n",
    "        train_loader=train_loader, valid_loader=test_loader, num_epochs=5, tboard=writer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = [train_loss, val_loss, loss_iters, valid_acc]\n",
    "save_model(model, optimizer, 5, stats, path=\"Lab Work/CUDA_Assignment_4/\", name=\"simpleCustomRNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1189766"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ActionRecognitionModel(input_channels=3, hidden_channels=128, kernel_size=3, feature_dim=64, output_dim=6)\n",
    "count_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActionRecognitionModel(\n",
       "  (encoder): ConvEncoder(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (4): ReLU()\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (conv_lstm): ConvLSTMCell(\n",
       "    (conv_i): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_f): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_c): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_o): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): AdaptiveAvgPool2d(output_size=1)\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): Linear(in_features=128, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "# classification loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Decay LR by a factor of 0.2 every 5 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBOARD_LOGS = os.path.join(os.getcwd(),\"Lab Work\", \"CUDA_Assignment_4\", \"tboard_logs\", \"RNNs\", \"FullConvLSTM_32spatial\")\n",
    "if not os.path.exists(TBOARD_LOGS):\n",
    "    os.makedirs(TBOARD_LOGS)\n",
    "\n",
    "shutil.rmtree(TBOARD_LOGS)\n",
    "writer = SummaryWriter(TBOARD_LOGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████▌                                                                                                                      | 1/5 [07:06<28:26, 426.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    Train loss: 0.55114\n",
      "    Valid loss: 1.79468\n",
      "    Accuracy: 14.688572272592998%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [35:31<00:00, 426.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "    Train loss: 0.07614\n",
      "    Valid loss: 1.36215\n",
      "    Accuracy: 72.95089417592656%\n",
      "\n",
      "\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss, loss_iters, valid_acc = train_model(\n",
    "        model=model, optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n",
    "        train_loader=train_loader, valid_loader=test_loader, num_epochs=5, tboard=writer\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = [train_loss, val_loss, loss_iters, valid_acc]\n",
    "save_model(model, optimizer, 5, path=\"Lab Work/CUDA_Assignment_4/\", name=\"FullConvLSTM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
