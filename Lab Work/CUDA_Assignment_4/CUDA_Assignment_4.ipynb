{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets\n",
    "\n",
    "loading from `/home/nfs/inf6/data/datasets/kth_actions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define the split for training and eval sets\n",
    "train_persons = {'person11', 'person12', 'person13', 'person14', 'person15', 'person16', 'person17', 'person02', 'person03', 'person05', 'person06', 'person07', 'person08', 'person09', 'person10', 'person18'}\n",
    "val_persons = {'person19', 'person20', 'person21', 'person23', 'person24', 'person25', 'person01', 'person04'}\n",
    "\n",
    "def get_sequences_with_labels(base_dir, persons):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    classes = os.listdir(base_dir)\n",
    "    class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        for person in persons:\n",
    "            person_sequences = glob(os.path.join(base_dir, cls, f'{person}*'))\n",
    "            sequences.extend(person_sequences)\n",
    "            labels.extend([class_to_idx[cls]] * len(person_sequences))\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "base_dir = '/home/nfs/inf6/data/datasets/kth_actions/processed'\n",
    "train_sequences, train_labels = get_sequences_with_labels(base_dir, train_persons)\n",
    "val_sequences, val_labels = get_sequences_with_labels(base_dir, val_persons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_frames(sequence_path):\n",
    "    frame_files = sorted(glob(os.path.join(sequence_path, '*.png')))\n",
    "    frames = [cv2.imread(frame_file) for frame_file in frame_files]\n",
    "    frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in frames]  # Convert to RGB\n",
    "    frames = [frame / 255.0 for frame in frames]  # Normalize\n",
    "    return frames\n",
    "\n",
    "def create_subsequences(frames, subsequence_length=13):\n",
    "    subsequences = []\n",
    "    for i in range(len(frames) - subsequence_length + 1):\n",
    "        subsequences.append(frames[i:i + subsequence_length])\n",
    "    return subsequences\n",
    "\n",
    "sequence_path = train_sequences[0]\n",
    "frames = load_frames(sequence_path)\n",
    "subsequences = create_subsequences(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, transform=None, subsequence_length=13):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.subsequence_length = subsequence_length\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        data = []\n",
    "        for sequence, label in zip(self.sequences, self.labels):\n",
    "            frames = load_frames(sequence)\n",
    "            subsequences = create_subsequences(frames, self.subsequence_length)\n",
    "            for subsequence in subsequences:\n",
    "                data.append((subsequence, label))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subsequence, label = self.data[idx]\n",
    "        if self.transform:\n",
    "            subsequence = [self.transform(frame) for frame in subsequence]\n",
    "        subsequence = torch.stack([torch.tensor(frame) for frame in subsequence])  # Convert to CxHxW\n",
    "        label = torch.tensor(label)\n",
    "        return subsequence, label\n",
    "\n",
    "# Define transforms if needed\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.RandomResizedCrop(224),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.Resize(256),\n",
    "        # transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = VideoDataset(train_sequences, train_labels, transform=data_transforms['train'])\n",
    "val_dataset = VideoDataset(val_sequences, val_labels, transform=data_transforms['val'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = 4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_426327/3716617533.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  subsequence = torch.stack([torch.tensor(frame) for frame in subsequence])  # Convert to CxHxW\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom LSTM  \n",
    "Source: https://github.com/piEsposito/pytorch-lstm-by-hand/blob/master/LSTM.ipynb - By Piero Esposito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Forget gate weights\n",
    "        self.W_f = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Input gate weights\n",
    "        self.W_i = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Candidate cell state weights\n",
    "        self.W_c = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Output gate weights\n",
    "        self.W_o = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.W_f, a=0.01)\n",
    "        nn.init.kaiming_uniform_(self.W_i, a=0.01)\n",
    "        nn.init.kaiming_uniform_(self.W_c, a=0.01)\n",
    "        nn.init.kaiming_uniform_(self.W_o, a=0.01)\n",
    "        nn.init.constant_(self.b_f, 0)\n",
    "        nn.init.constant_(self.b_i, 0)\n",
    "        nn.init.constant_(self.b_c, 0)\n",
    "        nn.init.constant_(self.b_o, 0)\n",
    "\n",
    "    def forward(self, input, layers=1, mode=\"random\"):\n",
    "        if mode == \"zeroes\":\n",
    "            hx = (torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device),\n",
    "                  torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device))\n",
    "        elif mode == \"Random\":\n",
    "            hx = (torch.rand(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device),\n",
    "                  torch.rand(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device))\n",
    "\n",
    "\n",
    "        h_prev, c_prev = hx\n",
    "        combined = torch.cat((input, h_prev), dim=1)\n",
    "\n",
    "        # Forget gate\n",
    "        f = torch.sigmoid(torch.matmul(combined, self.W_f) + self.b_f)\n",
    "\n",
    "        # Input gate\n",
    "        i = torch.sigmoid(torch.matmul(combined, self.W_i) + self.b_i)\n",
    "\n",
    "        # Candidate cell state\n",
    "        c_hat = torch.tanh(torch.matmul(combined, self.W_c) + self.b_c)\n",
    "\n",
    "        # Updated cell state\n",
    "        c = f * c_prev + i * c_hat\n",
    "\n",
    "        # Output gate\n",
    "        o = torch.sigmoid(torch.matmul(combined, self.W_o) + self.b_o)\n",
    "\n",
    "        # Updated hidden state\n",
    "        h = o * torch.tanh(c)\n",
    "\n",
    "        return h, c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
